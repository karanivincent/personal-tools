{
    "url": "https://microsoft.github.io/autogen/blog/page/3",
    "title": "No Title",
    "sections": [
        {
            "title": "Achieve More, Pay Less - Use GPT-4 Smartly",
            "content": [
                {
                    "text": "\n\nTL;DR:\n\nGPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark,\nHumanEval\n, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?\n\nIn this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward."
                }
            ],
            "subsections": []
        },
        {
            "title": "Observations\n​",
            "content": [
                {
                    "text": "The obstacle of leveraging these observations is that we do not know\na priori\nwhich tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.\n\nTo overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:"
                },
                {
                    "code": {
                        "language": "python",
                        "script": "def\nvowels_count\n(\ns\n)\n:\n\"\"\"Write a function vowels_count which takes a string representing\na word as input and returns the number of vowels in the string.\nVowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a\nvowel, but only when it is at the end of the given word.\nExample:\n>>> vowels_count(\"abcde\")\n2\n>>> vowels_count(\"ACEDY\")\n3\n\"\"\""
                    }
                },
                {
                    "text": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.\n\nWhat else can we do? We notice that:\nIt's \"easier\" to verify a given solution than finding a correct solution from scratch.\n\nSome simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code."
                }
            ],
            "subsections": []
        },
        {
            "title": "Solution\n​",
            "content": [
                {
                    "text": "Combining these observations, we can design a solution with two intuitive ideas:\n\n\n\nThis solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.\n\nAn implementation of this solution is provided in\nautogen\n. It uses the following sequence of configurations:"
                }
            ],
            "subsections": []
        },
        {
            "title": "Experiment Results\n​",
            "content": [
                {
                    "text": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.\nThe inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.\n\nHere are a few examples of function definitions which are solved by different configurations in the portfolio."
                },
                {
                    "code": {
                        "language": "python",
                        "script": "def\ncompare\n(\ngame\n,\nguess\n)\n:\n\"\"\"I think we all remember that feeling when the result of some long-awaited\nevent is finally known. The feelings and thoughts you have at that moment are\ndefinitely worth noting down and comparing.\nYour task is to determine if a person correctly guessed the results of a number of matches.\nYou are given two arrays of scores and guesses of equal length, where each index shows a match.\nReturn an array of the same length denoting how far off each guess was. If they have guessed correctly,\nthe value is 0, and if not, the value is the absolute difference between the guess and the score.\nexample:\ncompare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]\ncompare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]\n\"\"\""
                    }
                },
                {
                    "code": {
                        "language": "python",
                        "script": "def\nstring_xor\n(\na\n:\nstr\n,\nb\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\" Input are two strings a and b consisting only of 1s and 0s.\nPerform binary XOR on these inputs and return result also as a string.\n>>> string_xor('010', '110')\n'100'\n\"\"\""
                    }
                },
                {
                    "code": {
                        "language": "python",
                        "script": "def\nis_palindrome\n(\nstring\n:\nstr\n)\n-\n>\nbool\n:\n\"\"\" Test if given string is a palindrome \"\"\"\nreturn\nstring\n==\nstring\n[\n:\n:\n-\n1\n]\ndef\nmake_palindrome\n(\nstring\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\" Find the shortest palindrome that begins with a supplied string.\nAlgorithm idea is simple:\n- Find the longest postfix of supplied string that is a palindrome.\n- Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\n>>> make_palindrome('')\n''\n>>> make_palindrome('cat')\n'catac'\n>>> make_palindrome('cata')\n'catac'\n\"\"\""
                    }
                },
                {
                    "code": {
                        "language": "python",
                        "script": "def\nsort_array\n(\narr\n)\n:\n\"\"\"\nIn this Kata, you have to sort an array of non-negative integers according to\nnumber of ones in their binary representation in ascending order.\nFor similar number of ones, sort based on decimal value.\nIt must be implemented like this:\n>>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]\n>>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]\n>>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]\n\"\"\""
                    }
                },
                {
                    "text": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:\n\nIt is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.\n\nAn example notebook to run this experiment can be found at:\nhttps://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb\n. The experiment was run when AutoGen was a subpackage in FLAML."
                }
            ],
            "subsections": []
        },
        {
            "title": "Discussion\n​",
            "content": [
                {
                    "text": "Our solution is quite simple to implement using a generic interface offered in\nautogen\n, yet the result is quite encouraging.\n\nWhile the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:\n\nA\nprevious blog post\nprovides evidence that these ideas are relevant in solving math problems too.\nautogen\nuses a technique\nEcoOptiGen\nto support inference parameter tuning and model selection.\n\nThere are many directions of extensions in research and development:\n\nDo you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our\nDiscord\nserver for discussion."
                }
            ],
            "subsections": []
        },
        {
            "title": "For Further Reading\n​",
            "content": [],
            "subsections": []
        },
        {
            "title": "Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH",
            "content": [
                {
                    "text": "\n\nTL;DR:\n\nLarge language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?\n\nIn this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for\nMATH\n, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.\n\nWe will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.\n\nWe will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results."
                }
            ],
            "subsections": []
        },
        {
            "title": "Experiment Setup\n​",
            "content": [
                {
                    "text": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:\n\nWe adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:\n\nIn this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed."
                }
            ],
            "subsections": []
        },
        {
            "title": "Experiment Results\n​",
            "content": [
                {
                    "text": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.\n\nSurprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.\nThe same observation can be obtained on the level 3 Algebra test set.\n\n\n\nHowever, the selected model changes on level 4 Algebra.\n\n\n\nThis time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.\nOn level 5 the result is similar.\n\n\n\nWe can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.\n\nAn example notebook to run these experiments can be found at:\nhttps://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb\n. The experiments were run when AutoGen was a subpackage in FLAML."
                }
            ],
            "subsections": []
        },
        {
            "title": "Analysis and Discussion\n​",
            "content": [
                {
                    "text": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.\n\nThere are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via\nflaml.tune\n.\n\nThe need for model selection, parameter tuning and cost saving is not specific to the math problems. The\nAuto-GPT\nproject is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls."
                }
            ],
            "subsections": []
        },
        {
            "title": "For Further Reading\n​",
            "content": [
                {
                    "text": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our\nDiscord\nserver for discussion."
                }
            ],
            "subsections": []
        }
    ],
    "images": []
}