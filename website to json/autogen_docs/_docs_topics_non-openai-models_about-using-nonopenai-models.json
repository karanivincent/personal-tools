{
    "url": "https://microsoft.github.io/autogen/docs/topics/non-openai-models/about-using-nonopenai-models",
    "title": "Non-OpenAI Models",
    "sections": [
        {
            "title": "",
            "content": [
                {
                    "text": "AutoGen allows you to use non-OpenAI models through proxy servers that provide\nan OpenAI-compatible API or a\ncustom model client\nclass.\n\nBenefits of this flexibility include access to hundreds of models, assigning specialized\nmodels to agents (e.g., fine-tuned coding models), the ability to run AutoGen entirely\nwithin your environment, utilising both OpenAI and non-OpenAI models in one system, and cost\nreductions in inference."
                }
            ],
            "subsections": []
        },
        {
            "title": "OpenAI-compatible API proxy server\n​",
            "content": [
                {
                    "text": "Any proxy server that provides an API that is compatible with\nOpenAI's API\nwill work with AutoGen.\n\nThese proxy servers can be cloud-based or running locally within your environment.\n\n"
                }
            ],
            "subsections": [
                {
                    "title": "Cloud-based proxy servers\n​",
                    "content": [
                        {
                            "text": "By using cloud-based proxy servers, you are able to use models without requiring the hardware\nand software to run them.\n\nThese providers can host open source/weight models, like\nHugging Face\nand\nMistral AI\n,\nor their own closed models.\n\nWhen cloud-based proxy servers provide an OpenAI-compatible API, using them in AutoGen\nis straightforward. With\nLLM Configuration\ndone in\nthe same way as when using OpenAI's models, the primary difference is typically the\nauthentication which is usually handled through an API key.\n\nExamples of using cloud-based proxy servers providers that have an OpenAI-compatible API\nare provided below:"
                        }
                    ],
                    "subsections": []
                },
                {
                    "title": "Locally run proxy servers\n​",
                    "content": [
                        {
                            "text": "An increasing number of LLM proxy servers are available for use locally. These can be\nopen-source (e.g., LiteLLM, Ollama, vLLM) or closed-source (e.g., LM Studio), and are\ntypically used for running the full-stack within your environment.\n\nSimilar to cloud-based proxy servers, as long as these proxy servers provide an\nOpenAI-compatible API, running them in AutoGen is straightforward.\n\nExamples of using locally run proxy servers that have an OpenAI-compatible API are\nprovided below:\n\nIf you are planning to use Function Calling, not all cloud-based and local proxy servers support\nFunction Calling with their OpenAI-compatible API, so check their documentation."
                        }
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Custom Model Client class\n​",
            "content": [
                {
                    "text": "For more advanced users, you can create your own custom model client class, enabling\nyou to define and load your own models.\n\nSee the\nAutoGen with Custom Models: Empowering Users to Use Their Own Inference Mechanism\nblog post and\nthis notebook\nfor a guide to creating custom model client classes."
                }
            ],
            "subsections": []
        }
    ],
    "images": []
}